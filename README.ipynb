{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goodreads visualization\n",
    "\n",
    "An ipython notebook to play around with Goodreads data and make some seaborn visualizations, learn more about scikit-learn, my own playground!\n",
    "\n",
    "You can use it with your own data - go [here](https://www.goodreads.com/review/import) and press \"Export your library\" to get your own csv.\n",
    "\n",
    "The text you're reading is generated from a jupyter notebook by the Makefile. If you want to run it yourself, clone the repository then run\n",
    "\n",
    "    ipython3 notebook your_file.ipynb\n",
    "    \n",
    "to get the interactive version. In there, replace the path to my Goodreads exported file by yours in the ipynb file, and then run click on Cell -> Run All.\n",
    "\n",
    "## Dependencies\n",
    "\n",
    "* Python 3\n",
    "* Ipython3/Jupyter\n",
    "\n",
    "### Python packages\n",
    "\n",
    "* seaborn\n",
    "* pandas\n",
    "* wordcloud\n",
    "* nltk\n",
    "* networkx\n",
    "* pymarkovchain\n",
    "* scikit-learn\n",
    "* distance\n",
    "* image (PIL inside python for some weird reason)\n",
    "\n",
    "To install all:\n",
    "\n",
    "    sudo pip install seaborn wordcloud nltk networkx pymarkovchain image \n",
    "\n",
    "## Licenses\n",
    "\n",
    "License for reviews: CC-BY-SA 4.0\n",
    "Code: MIT\n",
    "\n",
    "OK, let's start!\n",
    "\n",
    "## Setting up the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "% pylab inline\n",
    "\n",
    "\n",
    "# for most plots\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "\n",
    "# for stats\n",
    "import scipy.stats\n",
    "\n",
    "# for time-related plots\n",
    "import datetime\n",
    "import calendar\n",
    "\n",
    "# for word cloud\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# for Markov chain\n",
    "from pymarkovchain import MarkovChain\n",
    "import pickle\n",
    "import networkx as nx\n",
    "\n",
    "# for shelf clustering\n",
    "import distance\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "sns.set_palette(\"coolwarm\")\n",
    "\n",
    "# change some plotting defaults\n",
    "rcParams[\"figure.figsize\"] = [14, 9]\n",
    "rcParams[\"axes.labelsize\"] = 15.0\n",
    "rcParams[\"axes.titlesize\"] = 15.0\n",
    "rcParams['xtick.labelsize'] = 15\n",
    "rcParams['ytick.labelsize'] = 15\n",
    "rcParams['font.size'] = 15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./goodreads_export.csv')\n",
    "# keep only books that have a rating (unrated books have a rating of 0, we don't need that)\n",
    "cleaned_df = df[df[\"My Rating\"] != 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Score distribution\n",
    "With a score scale of 1-5, you'd expect that the average score is ~~2.5~~ 3 (since 0 is not counted) after a few hundred books (in other words, is it a normal distribution?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.distplot(cleaned_df[\"My Rating\"], kde=False)\n",
    "\"Average: %.2f\"%cleaned_df[\"My Rating\"].mean(), \"Median: %s\"%cleaned_df[\"My Rating\"].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look normally distributed to me - let's ask Shapiro-Wilk (null hypothesis: data is drawn from normal distribution):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "W, p_value = scipy.stats.shapiro(cleaned_df[\"My Rating\"])\n",
    "if p_value < 0.05:\n",
    "    print(\"Rejecting null hypothesis - data does not come from a normal distribution (p=%s)\"%p_value)\n",
    "else:\n",
    "    print(\"Cannot reject null hypothesis (p=%s)\"%p_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In my case, the data is not normally distributed (in other words, the book scores are not evenly distributed around the middle). If you think about it, this makes sense: most readers don't read perfectly randomly, I avoid books I believe I'd dislike, and choose books that I prefer. I rate those books higher than average, therefore, my curve of scores is slanted towards the right."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot Pages vs Ratings\n",
    "\n",
    "Do I give longer books better scores? A minor tendency but nothing special (it's confounded by having just 5 possible numbers in ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "g = sns.jointplot(\"Number of Pages\", \"My Rating\", data=cleaned_df, kind=\"reg\", size=7, ylim=[0.5,5.5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can plot the \"residuals\" (what's left after calculating the regression line in the above plot) to see how useful a regression is - regression is useful when your residuals are randomly distributed around the y=0 line, i.e., it's a good fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "sns.residplot(\"Number of Pages\", \"My Rating\", data=cleaned_df,\n",
    "              scatter_kws={\"s\": 80});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That doesn't look random to me, with quite a slant towards the negative space! Regression isn't useful here.\n",
    "\n",
    "I seem to mostly read books at around 200 to 300 pages so it's hard to tell whether I give longer books better ratings. It's also a nice example that in regards to linear regression, a p-value as tiny as this one doesn't mean much, the r-value is still bad.\n",
    "\n",
    "***\n",
    "\n",
    "## plot Ratings vs Bookshelves\n",
    "\n",
    "Let's parse ratings for books and make a violin plot for the 7 categories with the most rated books!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CATEGORIES = 7 # number of most crowded categories to plot\n",
    "\n",
    "# we have to fiddle a bit - we have to count the ratings by category, \n",
    "# since each book can have several comma-delimited categories\n",
    "# TODO: find a pandas-like way to do this\n",
    "\n",
    "shelves_ratings = defaultdict(list) # key: shelf-name, value: list of ratings\n",
    "shelves_counter = Counter() # counts how many books on each shelf\n",
    "shelves_to_names = defaultdict(list) # key: shelf-name, value: list of book names\n",
    "\n",
    "for index, row in cleaned_df.iterrows():\n",
    "    my_rating = row[\"My Rating\"]\n",
    "    if my_rating == 0:\n",
    "        continue\n",
    "    if pd.isnull(row[\"Bookshelves\"]):\n",
    "        continue\n",
    "\n",
    "    shelves = row[\"Bookshelves\"].split(\",\")\n",
    "\n",
    "    for s in shelves:\n",
    "        # empty shelf?\n",
    "        if not s: continue\n",
    "        s = s.strip() # I had \"non-fiction\" and \" non-fiction\"\n",
    "        shelves_ratings[s].append(my_rating)\n",
    "        shelves_counter[s] += 10\n",
    "        shelves_to_names[s].append(row.Title)\n",
    "\n",
    "names = []\n",
    "ratings = []\n",
    "for name, _ in shelves_counter.most_common(CATEGORIES):\n",
    "    for number in shelves_ratings[name]:\n",
    "        names.append(name)\n",
    "        ratings.append(number)\n",
    "\n",
    "full_table = pd.DataFrame({\"Category\":names, \"Rating\":ratings})\n",
    "\n",
    "sns.violinplot(x = \"Category\", y = \"Rating\", data=full_table)\n",
    "# older versions of seaborn throw up here with\n",
    "# TypeError: violinplot() missing 1 required positional argument: 'vals'\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is some *bad* SF out there.\n",
    "\n",
    "However, the sci-fi score looks normally distributed! Let's check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print full_table\n",
    "W, p_value = scipy.stats.shapiro(full_table[full_table[\"Category\"] == \"sci-fi\"][\"Rating\"])\n",
    "if p_value < 0.05:\n",
    "    print(\"Rejecting null hypothesis - data does not come from a normal distribution (p=%s)\"%p_value)\n",
    "else:\n",
    "    print(\"Cannot reject null hypothesis (p=%s)\"%p_value)\n",
    "sns.distplot(full_table[full_table[\"Category\"] == \"sci-fi\"][\"Rating\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does look like a close call, it's a bit skewed towards the rating of 4.\n",
    "\n",
    "At this point I wonder - since we can assign multiple 'shelves' (tags) to each book, do I have some tags that appear more often together than not? Let's use R!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from rpy2 import robjects\n",
    "\n",
    "all_shelves = shelves_counter.keys()\n",
    "names_dict = {} # key: shelf name, value: robjects.StrVector of names\n",
    "for c in all_shelves:\n",
    "    names_dict[c] = robjects.StrVector(shelves_to_names[c])\n",
    "\n",
    "names_dict = robjects.ListVector(names_dict)    \n",
    "%load_ext rpy2.ipython\n",
    "%R library(UpSetR)\n",
    "# by default, only 5 sets are considered, so change nsets\n",
    "\n",
    "%R -i names_dict -r 150 -w 900 -h 700 upset(fromList(names_dict), order.by = \"freq\", nsets = 9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most shelves are 'alone', but 'essays + non-fiction' and 'biography + non-fiction' show the biggest overlap.\n",
    "\n",
    "I may have messed up the categories, let's cluster them! Typos should cluster together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# get the Levenshtein distance between all shelf titles, normalise the distance by string length\n",
    "X = np.array([[float(distance.levenshtein(shelf_1,shelf_2))/max(len(shelf_1), len(shelf_2)) \\\n",
    "               for shelf_1 in all_shelves] for shelf_2 in all_shelves])\n",
    "# scale for clustering\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "# after careful fiddling I'm settling on eps=10\n",
    "clusters = DBSCAN(eps=10, min_samples=1).fit_predict(X)\n",
    "print('DBSCAN made %s clusters for %s shelves/tags.'%(len(set(clusters)), len(all_shelves)))\n",
    "\n",
    "cluster_dict = defaultdict(list)\n",
    "assert len(clusters) == len(all_shelves)\n",
    "for cluster_label, element in zip(clusters, all_shelves):\n",
    "    cluster_dict[cluster_label].append(element)\n",
    "    \n",
    "print('Clusters with more than one member:')\n",
    "for k in sorted(cluster_dict):\n",
    "    if len(cluster_dict[k]) > 1:\n",
    "        print k, cluster_dict[k]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ha, the classic Austria/Australia thing. Some clusters are problematic due to too-short label names (arab/art), some other clusters are good and show me that I made some mistakes in labeling! French and France should be together, Greece and Greek too. *Neat!*\n",
    "\n",
    "(Without normalising the distance by string length clusters like horror/body-horror don't appear.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotHistogramDistanceRead.py\n",
    "\n",
    "Let's check the \"dates read\" for each book read and plot the distance between books read in days - shows you how quickly you hop from book to book."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# first, transform to datetype and get rid of all invalid dates\n",
    "dates = pd.to_datetime(cleaned_df[\"Date Read\"])\n",
    "dates = dates.dropna()\n",
    "sorted_dates = sorted(dates)\n",
    "\n",
    "last_date = None\n",
    "differences = []\n",
    "all_days = []\n",
    "all_days_without_2012 = [] # not much goodreads usage in 2012 - remove that year\n",
    "for date in sorted_dates:\n",
    "    if not last_date:\n",
    "        last_date = date\n",
    "        if date.year != 2012:\n",
    "            last_date_not_2012 = date\n",
    "    difference = date - last_date\n",
    "    \n",
    "    days = difference.days\n",
    "    all_days.append(days)\n",
    "    if date.year != 2012:\n",
    "        all_days_without_2012.append(days)\n",
    "    last_date = date\n",
    "\n",
    "sns.distplot(all_days, axlabel=\"Distance in days between books read\", kde=True)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, sometimes I just add several at once and guesstimate the correct \"date read\".\n",
    "\n",
    "I didn't use Goodreads in 2012 much so let's see how it looks like without 2012:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sns.distplot(all_days_without_2012, axlabel=\"Distance in days between books read\")\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## plot Heatmap of dates read\n",
    "\n",
    "Parses the \"dates read\" for each book read, bins them by month, and makes a heatmap to show in which months I read more than in others. Also makes a lineplot for books read, split up by year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# we need a dataframe in this format:\n",
    "# year months books_read\n",
    "# I am sure there's some magic pandas function for this\n",
    "\n",
    "read_dict = defaultdict(int) # key: (year, month), value: count of books read\n",
    "for date in sorted_dates:\n",
    "    this_year = date.year\n",
    "    this_month = date.month\n",
    "    read_dict[ (this_year, this_month) ] += 1\n",
    "\n",
    "first_date = sorted_dates[0]\n",
    "\n",
    "first_year = first_date.year\n",
    "first_month = first_date.month\n",
    "\n",
    "todays_date = datetime.datetime.today()\n",
    "todays_year = todays_date.year\n",
    "todays_month = todays_date.month\n",
    "\n",
    "all_years = []\n",
    "all_months = []\n",
    "all_counts = []\n",
    "for year in range(first_year, todays_year+1):\n",
    "    for month in range(1, 13):\n",
    "        if (year == todays_year) and month > todays_month:\n",
    "            # don't count future months\n",
    "            # it's 2015-12 now so a bit hard to test\n",
    "            break\n",
    "        this_count = read_dict[ (year, month) ]\n",
    "        all_years.append(year)\n",
    "        all_months.append(month)\n",
    "        all_counts.append(this_count)\n",
    "\n",
    "# now get it in the format heatmap() wants\n",
    "df = pd.DataFrame( { \"month\":all_months, \"year\":all_years, \"books_read\":all_counts } )\n",
    "dfp = df.pivot(\"month\", \"year\", \"books_read\")\n",
    "\n",
    "# now make the heatmap\n",
    "ax = sns.heatmap(dfp, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What happened in May 2014?\n",
    "\n",
    "***\n",
    "\n",
    "## Plot books read by year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "g = sns.FacetGrid(df, col=\"year\", sharey=True, sharex=True) \n",
    "g.map(plt.plot, \"month\", \"books_read\")\n",
    "g.set_ylabels(\"Books read\")\n",
    "g.set_xlabels(\"Month\")\n",
    "pylab.xlim(1, 12)\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's nice how reading behaviour (Goodreads usage) connects over the months - it slowly in 2013, stays constant in 2014/2015, and now goes down again. You can see when my son was born!\n",
    "\n",
    "(Solution: 2016-8-25)\n",
    "\n",
    "***\n",
    "\n",
    "## plot Word Cloud\n",
    "\n",
    "\n",
    "This one removes noisy words and creates a word-cloud of most commonly used words in the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def replace_by_space(word):\n",
    "    new = []\n",
    "    for letter in word:\n",
    "        if letter in REMOVE:\n",
    "            new.append(' ')\n",
    "        else:\n",
    "            new.append(letter)\n",
    "    return ''.join(new)\n",
    "\n",
    "STOP = stopwords.words(\"english\")\n",
    "html_clean = re.compile('<.*?>')\n",
    "gr_clean = re.compile('\\[.*?\\]')\n",
    "PRINTABLE = string.printable\n",
    "REMOVE = set([\"!\",\"(\",\")\",\":\",\".\",\";\",\",\",'\"',\"?\",\"-\",\">\",\"_\"])\n",
    "\n",
    "all_my_words = []\n",
    "all_my_words_with_stop_words = []\n",
    "\n",
    "reviews = cleaned_df[\"My Review\"]\n",
    "\n",
    "num_reviews = 0\n",
    "num_words = 0\n",
    "for row in reviews:\n",
    "    if pd.isnull(row):\n",
    "        continue\n",
    "    review = row.lower()\n",
    "    if not review:\n",
    "        # empty review\n",
    "        continue\n",
    "    # clean strings\n",
    "    cleaned_review = re.sub(html_clean, '', review)\n",
    "    cleaned_review = re.sub(gr_clean, '', cleaned_review)\n",
    "    all_my_words_with_stop_words += cleaned_review\n",
    "    cleaned_review = replace_by_space(cleaned_review)\n",
    "    cleaned_review = \"\".join(filter(lambda x: x in PRINTABLE, cleaned_review))\n",
    "    # clean words\n",
    "    cleaned_review = cleaned_review.split()\n",
    "    cleaned_review = list(filter(lambda x: x not in STOP, cleaned_review))\n",
    "    num_words += len(cleaned_review)\n",
    "    all_my_words += cleaned_review\n",
    "    num_reviews += 1\n",
    "\n",
    "print(\"You have %s words in %s reviews\"%(num_words, num_reviews))\n",
    "\n",
    "# we need all words later for the Markov chain\n",
    "all_my_words_with_stop_words = ''.join(all_my_words_with_stop_words)\n",
    "\n",
    "# WordCloud takes only string, no list/set\n",
    "wordcloud = WordCloud(max_font_size=200, width=800, height=500).generate(' '.join(all_my_words))\n",
    "pylab.imshow(wordcloud)\n",
    "pylab.axis(\"off\")\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## plot books read vs. week-day\n",
    "\n",
    "Let's parse the weekday a \"book read\" has been added and count them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# initialize the dict in the correct order\n",
    "read_dict = OrderedDict() # key: weekday, value: count of books read\n",
    "for day in range(0,7):\n",
    "    read_dict[calendar.day_name[day]] = 0\n",
    "\n",
    "for date in sorted_dates:\n",
    "    weekday_name = calendar.day_name[date.weekday()]  # Sunday\n",
    "    read_dict[weekday_name] += 1\n",
    "\n",
    "full_table = pd.DataFrame({\"Weekday\":list(read_dict.keys()), \"Books read\":list(read_dict.values())})\n",
    "\n",
    "sns.barplot(x=\"Weekday\", y=\"Books read\", data=full_table)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Monday is procrastination day.\n",
    "\n",
    "***\n",
    "\n",
    "## Generate Reviews\n",
    "\n",
    "Tiny script that uses a simple Markov Chain and the review text as created by plotWordCloud.py to generate new reviews.\n",
    "Some examples:\n",
    "\n",
    "* “natural” death, almost by definition, means something slow, smelly and painful\n",
    "* a kind of cyborg, saved by the master was plagued in his work - for that i'm getting angry again just typing this - some are of exactly the opposite, and of black holes\n",
    "* american actress wikipedia tells me) once said: \"a critic never fights the battle; they just read, focus on his own goshawk 50 years\n",
    "* he always wanted to do something, and i don't know how accurate he is\n",
    "* not recommended for: people who, if they can't be reduced to a small essay\n",
    "* machiavelli summarises quite a bit like reading a 120 pages summary of the helmet of horror\n",
    "* - no supervisor, no grant attached to a beautiful suicide and now i cleared my mind of circe's orders -cramping my style, urging me not to write the paper\n",
    "* not being focused on useless mobile apps, but on medical companies that treat death as a sign of dissent\n",
    "* the harassment of irs-personnel to get into the dark cave\n",
    "\n",
    "*why does this work so well*\n",
    "\n",
    "This script also creates a graph of probabilities for word connections for the word \"translation\", the thicker the edge between the nodes, the higher the probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mc = MarkovChain(dbFilePath='./markov_db')\n",
    "mc.generateDatabase(all_my_words_with_stop_words)\n",
    "\n",
    "print(mc.generateString())\n",
    "\n",
    "mc.dumpdb()\n",
    "\n",
    "# a key in the datbase looks like:\n",
    "# ('when', 'you') defaultdict(<function _one at 0x7f5c843a4500>, \n",
    "# {'just': 0.06250000000059731, 'feel': 0.06250000000059731, 'had': 0.06250000000059731, 'accidentally': 0.06250000000059731, ''love': 0.06250000000059731, 'read': 0.06250000000059731, 'see': 0.06250000000059731, 'base': 0.06250000000059731, 'know': 0.12499999999641617, 'have': 0.12499999999641617, 'were': 0.06250000000059731, 'come': 0.06250000000059731, 'can't': 0.06250000000059731, 'are': 0.06250000000059731})\n",
    "# so 'just' follows after 'when you' with 6% probability\n",
    "\n",
    "db = pickle.load(open('./markov_db', 'rb'))\n",
    "# let's get a good node\n",
    "#for key in db:\n",
    "#    # has in between 5 and 10 connections\n",
    "#    if len(db[key]) > 5 and (len(db[key]) < 10):\n",
    "#        if len(set(db[key].values())) > 2:\n",
    "#            print key, set(db[key].values())\n",
    "\n",
    "# manually chosen from above\n",
    "good_key = ('translation',)\n",
    "values = db[good_key]\n",
    "\n",
    "# create the graph\n",
    "\n",
    "G = nx.DiGraph()\n",
    "good_key = str(good_key[0])\n",
    "G.add_node(good_key)\n",
    "G.add_nodes_from(values.keys())\n",
    "# get the graph for one of the connected nodes\n",
    "# we go only one step deep - anything more and we'd better use recursion (but graph gets ugly then anyway)\n",
    "for v in values:\n",
    "    if (v,) in db and (len(db[(v,)]) < 20):\n",
    "        G.add_nodes_from(db[(v,)].keys())\n",
    "        for partner in db[(v,)]:\n",
    "            edge_weight = db[(v,)][partner]\n",
    "            G.add_weighted_edges_from([ (v, partner, edge_weight) ])\n",
    "        # for now, only add one\n",
    "        break\n",
    "\n",
    "# now add the edges of the \"original\" graph around \"translation\"\n",
    "for partner in values:\n",
    "    edge_weight = values[partner]\n",
    "    G.add_weighted_edges_from([ (good_key, partner, edge_weight) ])\n",
    "\n",
    "pos = nx.spring_layout(G)\n",
    "\n",
    "nx.draw_networkx_nodes(G, pos, node_color = 'white', node_size = 2500)\n",
    "\n",
    "# width of edges is based on probability * 10\n",
    "for edge in G.edges(data=True):\n",
    "    nx.draw_networkx_edges(G, pos, edgelist = [(edge[0], edge[1])], width = edge[2]['weight']*10)\n",
    "\n",
    "nx.draw_networkx_labels(G, pos, font_size=10, font_family='sans-serif')\n",
    "pylab.axis('off')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I really wonder why it always forces the circular layout - it should connect from \"translation\" to \"(i\" which in turn connects to a few nodes.\n",
    "\n",
    "***\n",
    "\n",
    "## Some other ideas\n",
    "\n",
    "- Some people on goodreads have complained that their reviews disappear and I feel (but don't know) that I lost at least one, this tracks my exported CSV to check whether it actually happens. So far I haven't observed it.\n",
    "- ~~Write automated parser that exports reviews to html/epub/tumblr/blogger/wordpress etc.~~ support for this was added to goodreads)\n",
    "- ~~cron job which automatically pulls exported CSV from https://www.goodreads.com/review_porter/goodreads_export.csv (login a bit weird esp. with Facebook login, use API instead? Needs dev key, but easier to do /review/list.xml=USERID than to play Red Queen with Facebook's oauth)~~\n",
    "- various visualization things in regards to language use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
